{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/diya_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Prompts ===\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an air pollution assistant. \"\n",
    "    \"Strictly respond to queries with a single real number only. \"\n",
    "    \"Do not include any units, explanation, or punctuation. Just a single number.\"\n",
    ")\n",
    "\n",
    "USER_TEMPLATE = (\n",
    "    \"What is the average PM2.5 concentration (in μg/m³) in India during {month}, {year}? Give a single number only.\"\n",
    ")\n",
    "\n",
    "hf_token = \"Your Token\"\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma 9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Settings ===\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "\n",
    "# === Load Model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=dtype,\n",
    "    token=hf_token,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# === Query Function ===\n",
    "def query_llm(month, year):\n",
    "    user_prompt = USER_TEMPLATE.format(month=month, year=year)\n",
    "    full_prompt = f\"{SYSTEM_PROMPT}\\n\\n{user_prompt}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    match = re.search(r\"\\d+(\\.\\d+)?\", decoded)\n",
    "    return float(match.group()) if match else float(\"nan\")\n",
    "\n",
    "# === Save Results ===\n",
    "save_path = \"gemma2_9b_it_nation_level_2023.csv\"\n",
    "rows = []\n",
    "if os.path.exists(save_path):\n",
    "    existing_df = pd.read_csv(save_path)\n",
    "    print(f\"Resuming from saved file with {len(existing_df)} rows.\")\n",
    "    processed_months = set(existing_df[\"month\"])\n",
    "    rows = existing_df.to_dict(\"records\")\n",
    "else:\n",
    "    print(\"No previous file found. Starting fresh.\")\n",
    "    processed_months = set()\n",
    "\n",
    "months = list(range(1, 13))\n",
    "year = 2023\n",
    "\n",
    "for month_num in months:\n",
    "    month_name = datetime(1900, month_num, 1).strftime(\"%B\")\n",
    "\n",
    "    if month_name in processed_months:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prediction = query_llm(month_name, year)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM — stopping execution.\")\n",
    "            raise\n",
    "        else:\n",
    "            prediction = float(\"nan\")\n",
    "\n",
    "    row = {\n",
    "        \"year\": year,\n",
    "        \"month\": month_name,\n",
    "        \"model\": model_id,\n",
    "        \"pm2.5\": prediction,\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "    # Save after every month\n",
    "    try:\n",
    "        pd.DataFrame(rows).to_csv(save_path, index=False)\n",
    "        print(f\"[{datetime.now()}] Saved prediction for {month_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now()}] Save error after {month_name}: {e}\")\n",
    "\n",
    "print(f\"[{datetime.now()}] Final save complete. Total months: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma 27b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:11<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous file found. Starting fresh.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to cpu device (arg102_1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 00:00:42.668609] Saved prediction for January\n",
      "[2025-07-17 00:00:42.930735] Saved prediction for February\n",
      "[2025-07-17 00:00:43.190959] Saved prediction for March\n",
      "[2025-07-17 00:00:43.452111] Saved prediction for April\n",
      "[2025-07-17 00:00:43.712038] Saved prediction for May\n",
      "[2025-07-17 00:00:43.973564] Saved prediction for June\n",
      "[2025-07-17 00:00:44.233492] Saved prediction for July\n",
      "[2025-07-17 00:00:44.494085] Saved prediction for August\n",
      "[2025-07-17 00:00:44.755003] Saved prediction for September\n",
      "[2025-07-17 00:00:45.014928] Saved prediction for October\n",
      "[2025-07-17 00:00:45.320765] Saved prediction for November\n",
      "[2025-07-17 00:00:45.627398] Saved prediction for December\n",
      "[2025-07-17 00:00:45.627643] Final save complete. Total months: 12\n"
     ]
    }
   ],
   "source": [
    "# === Settings ===\n",
    "model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# === Load Model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=dtype,\n",
    "    token=hf_token,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# === Query Function ===\n",
    "def query_llm(month, year):\n",
    "    user_prompt = USER_TEMPLATE.format(month=month, year=year)\n",
    "    full_prompt = f\"{SYSTEM_PROMPT}\\n\\n{user_prompt}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    match = re.search(r\"\\d+(\\.\\d+)?\", decoded)\n",
    "    return float(match.group()) if match else float(\"nan\")\n",
    "\n",
    "# === Save Results ===\n",
    "save_path = \"gemma2_27b_it_nation_level_2023.csv\"\n",
    "rows = []\n",
    "if os.path.exists(save_path):\n",
    "    existing_df = pd.read_csv(save_path)\n",
    "    print(f\"Resuming from saved file with {len(existing_df)} rows.\")\n",
    "    processed_months = set(existing_df[\"month\"])\n",
    "    rows = existing_df.to_dict(\"records\")\n",
    "else:\n",
    "    print(\"No previous file found. Starting fresh.\")\n",
    "    processed_months = set()\n",
    "\n",
    "months = list(range(1, 13))\n",
    "year = 2023\n",
    "\n",
    "for month_num in months:\n",
    "    month_name = datetime(1900, month_num, 1).strftime(\"%B\")\n",
    "\n",
    "    if month_name in processed_months:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prediction = query_llm(month_name, year)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM — stopping execution.\")\n",
    "            raise\n",
    "        else:\n",
    "            prediction = float(\"nan\")\n",
    "\n",
    "    row = {\n",
    "        \"year\": year,\n",
    "        \"month\": month_name,\n",
    "        \"model\": model_id,\n",
    "        \"pm2.5\": prediction,\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "    # Save after every month\n",
    "    try:\n",
    "        pd.DataFrame(rows).to_csv(save_path, index=False)\n",
    "        print(f\"[{datetime.now()}] Saved prediction for {month_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now()}] Save error after {month_name}: {e}\")\n",
    "\n",
    "print(f\"[{datetime.now()}] Final save complete. Total months: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous file found. Starting fresh.\n",
      "[2025-07-17 00:06:07.539736] Saved prediction for January\n",
      "[2025-07-17 00:06:07.601367] Saved prediction for February\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 00:06:07.663103] Saved prediction for March\n",
      "[2025-07-17 00:06:07.724092] Saved prediction for April\n",
      "[2025-07-17 00:06:07.785185] Saved prediction for May\n",
      "[2025-07-17 00:06:07.845934] Saved prediction for June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 00:06:07.907152] Saved prediction for July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 00:06:08.163718] Saved prediction for August\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 00:06:08.420686] Saved prediction for September\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 00:06:08.677775] Saved prediction for October\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 00:06:08.934424] Saved prediction for November\n",
      "[2025-07-17 00:06:09.219120] Saved prediction for December\n",
      "[2025-07-17 00:06:09.219330] Final save complete. Total months: 12\n"
     ]
    }
   ],
   "source": [
    "# === Settings ===\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "save_path = \"llama3_8b_it_nation_level_2023.csv\"\n",
    "\n",
    "# === Load Model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Important for padding\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_token,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Optional: compile model if supported\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# === Inference Function ===\n",
    "def query_llm(month, year):\n",
    "    user_prompt = USER_TEMPLATE.format(month=month, year=year)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    match = re.search(r\"\\d+(\\.\\d+)?\", decoded)\n",
    "    return float(match.group()) if match else float(\"nan\")\n",
    "\n",
    "# === Resume or Initialize ===\n",
    "rows = []\n",
    "if os.path.exists(save_path):\n",
    "    existing_df = pd.read_csv(save_path)\n",
    "    print(f\"Resuming from saved file with {len(existing_df)} rows.\")\n",
    "    processed_months = set(existing_df[\"month\"])\n",
    "    rows = existing_df.to_dict(\"records\")\n",
    "else:\n",
    "    print(\"No previous file found. Starting fresh.\")\n",
    "    processed_months = set()\n",
    "\n",
    "months = list(range(1, 13))\n",
    "year = 2023\n",
    "\n",
    "for month_num in months:\n",
    "    month_name = datetime(1900, month_num, 1).strftime(\"%B\")\n",
    "\n",
    "    if month_name in processed_months:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prediction = query_llm(month_name, year)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM — stopping execution.\")\n",
    "            raise\n",
    "        else:\n",
    "            prediction = float(\"nan\")\n",
    "\n",
    "    row = {\n",
    "        \"year\": year,\n",
    "        \"month\": month_name,\n",
    "        \"model\": model_id,\n",
    "        \"pm2.5\": prediction,\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "    # Save after every month\n",
    "    try:\n",
    "        pd.DataFrame(rows).to_csv(save_path, index=False)\n",
    "        print(f\"[{datetime.now()}] Saved prediction for {month_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now()}] Save error after {month_name}: {e}\")\n",
    "\n",
    "print(f\"[{datetime.now()}] Final save complete. Total months: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:13<00:00,  2.30it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous file found. Starting fresh.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:29:03.881611] Saved prediction for January\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:29:20.150769] Saved prediction for February\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:29:36.455737] Saved prediction for March\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:29:52.786148] Saved prediction for April\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:30:09.147213] Saved prediction for May\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:30:25.462609] Saved prediction for June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:30:41.796033] Saved prediction for July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:30:58.095826] Saved prediction for August\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:31:14.396077] Saved prediction for September\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:31:30.727133] Saved prediction for October\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-24 21:31:47.081550] Saved prediction for November\n",
      "[2025-07-24 21:32:03.416624] Saved prediction for December\n",
      "[2025-07-24 21:32:03.417091] Final save complete. Total months: 12\n"
     ]
    }
   ],
   "source": [
    "# === Settings ===\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "save_path = \"AirQuality/RQ1/Dataset/llama3_70b_it_nation_level_2023.csv\"\n",
    "\n",
    "# === Load Model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0: \"78GiB\", \"cpu\": \"30GiB\"}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Optional: compile model if supported\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# === Inference Function ===\n",
    "def query_llm(month, year):\n",
    "    user_prompt = USER_TEMPLATE.format(month=month, year=year)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    match = re.search(r\"\\d+(\\.\\d+)?\", decoded)\n",
    "    return float(match.group()) if match else float(\"nan\")\n",
    "\n",
    "# === Resume or Initialize ===\n",
    "rows = []\n",
    "if os.path.exists(save_path):\n",
    "    existing_df = pd.read_csv(save_path)\n",
    "    print(f\"Resuming from saved file with {len(existing_df)} rows.\")\n",
    "    processed_months = set(existing_df[\"month\"])\n",
    "    rows = existing_df.to_dict(\"records\")\n",
    "else:\n",
    "    print(\"No previous file found. Starting fresh.\")\n",
    "    processed_months = set()\n",
    "\n",
    "months = list(range(1, 13))\n",
    "year = 2023\n",
    "\n",
    "for month_num in months:\n",
    "    month_name = datetime(1900, month_num, 1).strftime(\"%B\")\n",
    "\n",
    "    if month_name in processed_months:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prediction = query_llm(month_name, year)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM — stopping execution.\")\n",
    "            raise\n",
    "        else:\n",
    "            prediction = float(\"nan\")\n",
    "\n",
    "    row = {\n",
    "        \"year\": year,\n",
    "        \"month\": month_name,\n",
    "        \"model\": model_id,\n",
    "        \"pm2.5\": prediction,\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "    # Save after every month\n",
    "    try:\n",
    "        pd.DataFrame(rows).to_csv(save_path, index=False)\n",
    "        print(f\"[{datetime.now()}] Saved prediction for {month_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now()}] Save error after {month_name}: {e}\")\n",
    "\n",
    "print(f\"[{datetime.now()}] Final save complete. Total months: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen 32B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:12<00:00,  1.36it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous file found. Starting fresh.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:54.807880] Saved prediction for January\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:55.156748] Saved prediction for February\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:55.504862] Saved prediction for March\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:55.853671] Saved prediction for April\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:56.203984] Saved prediction for May\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:56.552584] Saved prediction for June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:56.901695] Saved prediction for July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:57.251374] Saved prediction for August\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:57.599824] Saved prediction for September\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:57.949762] Saved prediction for October\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-03 23:35:58.310243] Saved prediction for November\n",
      "[2025-08-03 23:35:58.598720] Saved prediction for December\n",
      "[2025-08-03 23:35:58.599008] Final save complete. Total months: 12\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === Settings ===\n",
    "model_id = \"Qwen/Qwen3-32B\"\n",
    "save_path = \"AirQuality/RQ1/Dataset/National/qwen3_32b_nation_level_2023.csv\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_token,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Optional: compile model if supported\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# === Inference Function ===\n",
    "def query_llm(month, year):\n",
    "    user_prompt = USER_TEMPLATE.format(month=month, year=year)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    output_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:].tolist()\n",
    "    decoded = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    match = re.search(r\"\\d+(\\.\\d+)?\", decoded)\n",
    "    return float(match.group()) if match else float(\"nan\")\n",
    "\n",
    "# === Resume or Initialize ===\n",
    "rows = []\n",
    "if os.path.exists(save_path):\n",
    "    existing_df = pd.read_csv(save_path)\n",
    "    print(f\"Resuming from saved file with {len(existing_df)} rows.\")\n",
    "    processed_months = set(existing_df[\"month\"])\n",
    "    rows = existing_df.to_dict(\"records\")\n",
    "else:\n",
    "    print(\"No previous file found. Starting fresh.\")\n",
    "    processed_months = set()\n",
    "\n",
    "months = list(range(1, 13))\n",
    "year = 2023\n",
    "\n",
    "for month_num in months:\n",
    "    month_name = datetime(1900, month_num, 1).strftime(\"%B\")\n",
    "\n",
    "    if month_name in processed_months:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prediction = query_llm(month_name, year)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM — stopping execution.\")\n",
    "            raise\n",
    "        else:\n",
    "            prediction = float(\"nan\")\n",
    "\n",
    "    row = {\n",
    "        \"year\": year,\n",
    "        \"month\": month_name,\n",
    "        \"model\": model_id,\n",
    "        \"pm2.5\": prediction,\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "    # Save after every month\n",
    "    try:\n",
    "        pd.DataFrame(rows).to_csv(save_path, index=False)\n",
    "        print(f\"[{datetime.now()}] Saved prediction for {month_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now()}] Save error after {month_name}: {e}\")\n",
    "\n",
    "print(f\"[{datetime.now()}] Final save complete. Total months: {len(rows)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diya_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
