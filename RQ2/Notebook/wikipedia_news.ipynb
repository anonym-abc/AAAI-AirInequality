{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import process, fuzz\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city', 'state', 'YearMonth', 'AT', 'BP', 'PM2.5', 'RF', 'VWS', 'WD',\n",
       "       'WS', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"AirQuality/Dataset/Ground_Truth_2023_Final.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 36/201 [00:32<02:22,  1.15it/s]/opt/anaconda3/envs/diya_env/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/envs/diya_env/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "100%|██████████| 201/201 [03:49<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "unique_cities = df[[\"city\", \"state\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "\n",
    "def city_has_wikipedia_page(city, state):\n",
    "    search_terms = [city.strip(), f\"{city.strip()}, {state.strip()}\"]\n",
    "    \n",
    "    for term in search_terms:\n",
    "        try:\n",
    "            page = wikipedia.page(term, auto_suggest=False)\n",
    "            return True\n",
    "        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):\n",
    "            continue\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "tqdm.pandas()\n",
    "unique_cities[\"has_wikipedia\"] = unique_cities.progress_apply(\n",
    "    lambda row: city_has_wikipedia_page(row[\"city\"], row[\"state\"]),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>has_wikipedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agartala</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agra</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aizawl</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ajmer</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Vrindavan</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Yadgir</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Yamuna Nagar</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              city           state  has_wikipedia\n",
       "0         Agartala         Tripura           True\n",
       "1             Agra   Uttar Pradesh           True\n",
       "2        Ahmedabad         Gujarat           True\n",
       "3           Aizawl         Mizoram           True\n",
       "4            Ajmer       Rajasthan           True\n",
       "..             ...             ...            ...\n",
       "196     Vijayawada  Andhra Pradesh           True\n",
       "197  Visakhapatnam  Andhra Pradesh           True\n",
       "198      Vrindavan   Uttar Pradesh           True\n",
       "199         Yadgir       Karnataka           True\n",
       "200   Yamuna Nagar         Haryana           True\n",
       "\n",
       "[201 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cities.loc[\n",
    "    (unique_cities[\"city\"].str.lower() == \"puducherry\".lower()) &\n",
    "    (unique_cities[\"state\"].str.lower() == \"puducherry\".lower()),\n",
    "    \"has_wikipedia\"\n",
    "] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Byrnihat</td>\n",
       "      <td>Assam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chhal</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kunjemura</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mandikhera</td>\n",
       "      <td>Haryana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Manguraha</td>\n",
       "      <td>Bihar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suakati</td>\n",
       "      <td>Odisha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tumidih</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city         state\n",
       "0    Byrnihat         Assam\n",
       "1       Chhal  Chhattisgarh\n",
       "2   Kunjemura  Chhattisgarh\n",
       "3  Mandikhera       Haryana\n",
       "4   Manguraha         Bihar\n",
       "5     Suakati        Odisha\n",
       "6     Tumidih  Chhattisgarh"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_wiki_cities = unique_cities[unique_cities[\"has_wikipedia\"] == False][[\"city\", \"state\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "missing_wiki_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki page length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 36/201 [01:08<05:16,  1.92s/it]/opt/anaconda3/envs/diya_env/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/envs/diya_env/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "100%|██████████| 201/201 [07:18<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "unique_cities = df[[\"city\", \"state\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def get_wikipedia_info(city, state):\n",
    "    search_terms = [city.strip(), f\"{city.strip()}, {state.strip()}\"]\n",
    "    \n",
    "    # Step 1: Try direct page fetch\n",
    "    for term in search_terms:\n",
    "        try:\n",
    "            page = wikipedia.page(term, auto_suggest=False)\n",
    "            return pd.Series([True, len(page.content)])\n",
    "        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):\n",
    "            continue\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Step 2: Try fuzzy matching from search results\n",
    "    try:\n",
    "        search_results = wikipedia.search(city)\n",
    "        if search_results:\n",
    "            # Pick the most similar result to city name\n",
    "            best_match, score, _ = process.extractOne(city, search_results, scorer=fuzz.token_sort_ratio)\n",
    "            if score > 70:  # threshold can be tuned\n",
    "                try:\n",
    "                    page = wikipedia.page(best_match, auto_suggest=True)\n",
    "                    return pd.Series([True, len(page.content)])\n",
    "                except:\n",
    "                    pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return pd.Series([False, 0])\n",
    "\n",
    "\n",
    "# Apply function\n",
    "tqdm.pandas()\n",
    "unique_cities[[\"has_wikipedia\", \"wiki_len\"]] = unique_cities.progress_apply(\n",
    "    lambda row: get_wikipedia_info(row[\"city\"], row[\"state\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Byrnihat</td>\n",
       "      <td>Assam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chhal</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kunjemura</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mandikhera</td>\n",
       "      <td>Haryana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Manguraha</td>\n",
       "      <td>Bihar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suakati</td>\n",
       "      <td>Odisha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tumidih</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city         state\n",
       "0    Byrnihat         Assam\n",
       "1       Chhal  Chhattisgarh\n",
       "2   Kunjemura  Chhattisgarh\n",
       "3  Mandikhera       Haryana\n",
       "4   Manguraha         Bihar\n",
       "5     Suakati        Odisha\n",
       "6     Tumidih  Chhattisgarh"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_wiki_cities = unique_cities[unique_cities[\"has_wikipedia\"] == False][[\"city\", \"state\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "missing_wiki_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['media', 'date', 'url', 'heading', 'content', 'other.author',\n",
      "       'other.top_image', 'other.category', 'city', 'year', 'state',\n",
      "       'district', 'matches'],\n",
      "      dtype='object')\n",
      "(17374, 13)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"AirQuality/RQ2/Dataset/News_articles_dataset.csv.gz\"\n",
    "news_df = pd.read_csv(file_path)\n",
    "\n",
    "print(news_df.columns)\n",
    "print(news_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['media', 'date', 'url', 'heading', 'content', 'other.author',\n",
       "       'other.top_image', 'other.category', 'city', 'year', 'state',\n",
       "       'district', 'matches'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['city'] = news_df['city'].str.lower().str.strip()\n",
    "unique_cities['city'] = unique_cities['city'].str.lower().str.strip()\n",
    "\n",
    "news_city_counts = news_df['city'].value_counts().to_dict()\n",
    "unique_news_cities = list(news_city_counts.keys())\n",
    "\n",
    "def get_best_match(city_name):\n",
    "    result = process.extractOne(city_name, unique_news_cities, score_cutoff=80)\n",
    "    if result is not None:\n",
    "        match, score, _ = result  \n",
    "        return match, news_city_counts.get(match, 0)\n",
    "    else:\n",
    "        return None, 0\n",
    "\n",
    "unique_cities[['matched_news_city', 'media_count']] = unique_cities['city'].apply(\n",
    "    lambda x: pd.Series(get_best_match(x))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['city'] = news_df['city'].str.lower().str.strip()\n",
    "news_df['state'] = news_df['state'].str.lower().str.strip()\n",
    "unique_cities['city'] = unique_cities['city'].str.lower().str.strip()\n",
    "unique_cities['state'] = unique_cities['state'].str.lower().str.strip()\n",
    "\n",
    "news_city_state_counts = (\n",
    "    news_df.groupby(['city', 'state'])\n",
    "    .size()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "available_city_state_pairs = list(news_city_state_counts.keys())\n",
    "\n",
    "def get_best_match(city, state):\n",
    "    result = process.extractOne(\n",
    "        (city, state), \n",
    "        available_city_state_pairs, \n",
    "        scorer=fuzz.token_sort_ratio, \n",
    "        score_cutoff=80\n",
    "    )\n",
    "    if result:\n",
    "        match, score, _ = result\n",
    "        return f\"{match[0]}, {match[1]}\", news_city_state_counts.get(match, 0)\n",
    "    else:\n",
    "        return None, 0\n",
    "\n",
    "unique_cities[['matched_news_city', 'media_count']] = unique_cities.apply(\n",
    "    lambda row: pd.Series(get_best_match(row['city'], row['state'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city', 'state', 'has_wikipedia', 'wiki_len', 'matched_news_city',\n",
       "       'media_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>has_wikipedia</th>\n",
       "      <th>wiki_len</th>\n",
       "      <th>matched_news_city</th>\n",
       "      <th>media_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agartala</td>\n",
       "      <td>tripura</td>\n",
       "      <td>True</td>\n",
       "      <td>30333</td>\n",
       "      <td>agartala, tripura</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agra</td>\n",
       "      <td>uttar pradesh</td>\n",
       "      <td>True</td>\n",
       "      <td>51925</td>\n",
       "      <td>agra, uttar pradesh</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ahmedabad</td>\n",
       "      <td>gujarat</td>\n",
       "      <td>True</td>\n",
       "      <td>45827</td>\n",
       "      <td>ahmedabad, gujarat</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aizawl</td>\n",
       "      <td>mizoram</td>\n",
       "      <td>True</td>\n",
       "      <td>15347</td>\n",
       "      <td>aizawl, mizoram</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ajmer</td>\n",
       "      <td>rajasthan</td>\n",
       "      <td>True</td>\n",
       "      <td>16162</td>\n",
       "      <td>ajmer, rajasthan</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city          state  has_wikipedia  wiki_len    matched_news_city  \\\n",
       "0   agartala        tripura           True     30333    agartala, tripura   \n",
       "1       agra  uttar pradesh           True     51925  agra, uttar pradesh   \n",
       "2  ahmedabad        gujarat           True     45827   ahmedabad, gujarat   \n",
       "3     aizawl        mizoram           True     15347      aizawl, mizoram   \n",
       "4      ajmer      rajasthan           True     16162     ajmer, rajasthan   \n",
       "\n",
       "   media_count  \n",
       "0          1.0  \n",
       "1         88.0  \n",
       "2        239.0  \n",
       "3          1.0  \n",
       "4          3.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cities.to_csv(\"Wiki_News_Data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    media_count  num_cities\n",
      "0           0.0         133\n",
      "1           1.0          12\n",
      "2           2.0           7\n",
      "3           3.0           3\n",
      "4           4.0           1\n",
      "5           5.0           4\n",
      "6           7.0           2\n",
      "7          12.0           1\n",
      "8          21.0           1\n",
      "9          26.0           1\n",
      "10         30.0           1\n",
      "11         32.0           1\n",
      "12         39.0           2\n",
      "13         41.0           2\n",
      "14         43.0           1\n",
      "15         44.0           1\n",
      "16         46.0           1\n",
      "17         51.0           2\n",
      "18         56.0           1\n",
      "19         68.0           1\n",
      "20         74.0           1\n",
      "21         86.0           2\n",
      "22         88.0           1\n",
      "23         94.0           1\n",
      "24        121.0           1\n",
      "25        160.0           1\n",
      "26        204.0           1\n",
      "27        205.0           1\n",
      "28        239.0           1\n",
      "29        317.0           1\n",
      "30        320.0           1\n",
      "31        368.0           1\n",
      "32        386.0           1\n",
      "33        396.0           1\n",
      "34        471.0           1\n",
      "35        488.0           2\n",
      "36        537.0           1\n",
      "37        621.0           1\n",
      "38        647.0           1\n",
      "39        849.0           1\n",
      "40       5285.0           1\n"
     ]
    }
   ],
   "source": [
    "df_unique_cities = df.drop_duplicates(subset=[\"city\", \"state\"])\n",
    "\n",
    "media_count_stats = unique_cities[\"media_count\"].value_counts().reset_index()\n",
    "media_count_stats.columns = [\"media_count\", \"num_cities\"]\n",
    "media_count_stats = media_count_stats.sort_values(\"media_count\").reset_index(drop=True)\n",
    "print(media_count_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0, Max: 5285.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Min: {unique_cities['media_count'].min()}, Max: {unique_cities['media_count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cities.to_csv(\"Wiki_News.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diya_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
